"""
Enhanced Google Gemini AI service for advanced misinformation detection.
Implements structured claim extraction, evidence-grounded retrieval, and stance analysis.
"""
import time
import json
import logging
import hashlib
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Union
import google.generativeai as genai
from PIL import Image
import io
import numpy as np
import httpx

from app.core.config import settings
from app.models.schemas import (
    ContentType,
    MisinformationLevel,
    DetectionExplanation,
    SourceInfo,
    ContentAnalysisResponse,
)
from app.models.misinformation_schemas import (
    MisinformationAnalysisRequest,
    MisinformationAnalysisResponse,
    ClaimExtraction,
    EvidenceCitation,
    StanceAnalysis,
    StanceType,
    CredibilityBadgeType,
    ManipulationTechnique,
    ProcessingModel,
    LanguageCode,
    LearnCard,
    CacheEntry,
    FactCheckAPIResponse,
    EvidenceRetrievalResult,
    ClaimAnalysisStep,
    DetailedAnalysisLog,
)

logger = logging.getLogger(__name__)


class EnhancedGeminiService:
    """Advanced service for misinformation detection using Google Gemini AI."""
    
    def __init__(self):
        """Initialize enhanced Gemini service with multiple models and caching."""
        try:
            genai.configure(api_key=settings.GEMINI_API_KEY)
            
            # Initialize multiple models for efficiency and escalation
            self.flash_model = genai.GenerativeModel(settings.VERTEX_AI_MODEL_GEMINI_FLASH)
            self.pro_model = genai.GenerativeModel(settings.VERTEX_AI_MODEL_GEMINI_PRO)
            self.vision_model = genai.GenerativeModel(settings.GEMINI_VISION_MODEL)
            
            # Initialize embedding model for context understanding
            self.embedding_model = "models/embedding-001"
            
            # In-memory cache for results (in production, use Redis or similar)
            self.analysis_cache: Dict[str, CacheEntry] = {}
            self.cache_ttl_hours = 24  # Cache validity in hours
            
            # External API clients
            self.fact_check_api_key = settings.FACT_CHECK_API_KEY
            self.http_client = httpx.AsyncClient(timeout=30.0)
            
            logger.info("âœ… Enhanced Gemini service initialized successfully")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize Enhanced Gemini service: {str(e)}")
            self.flash_model = None
            self.pro_model = None
            self.vision_model = None
            self.embedding_model = None
    
    def _generate_content_hash(self, content: str, content_type: str = "text") -> str:
        """Generate SHA-256 hash for content caching."""
        content_str = f"{content_type}:{content}"
        return hashlib.sha256(content_str.encode()).hexdigest()
    
    def _get_cached_analysis(self, content_hash: str) -> Optional[MisinformationAnalysisResponse]:
        """Retrieve cached analysis if available and not expired."""
        if content_hash not in self.analysis_cache:
            return None
        
        cache_entry = self.analysis_cache[content_hash]
        
        # Check if cache entry is expired
        cache_age = datetime.now() - cache_entry.created_at
        if cache_age > timedelta(hours=self.cache_ttl_hours):
            del self.analysis_cache[content_hash]
            return None
        
        # Update access statistics
        cache_entry.access_count += 1
        cache_entry.last_accessed = datetime.now()
        
        # Mark as cache hit
        result = cache_entry.result
        result.cache_hit = True
        
        logger.info(f"âœ… Cache hit for content hash: {content_hash[:16]}...")
        return result
    
    def _cache_analysis(self, content_hash: str, result: MisinformationAnalysisResponse) -> None:
        """Cache analysis result."""
        cache_entry = CacheEntry(
            content_hash=content_hash,
            result=result,
            created_at=datetime.now(),
            access_count=1,
            last_accessed=datetime.now()
        )
        self.analysis_cache[content_hash] = cache_entry
        logger.info(f"âœ… Cached analysis for content hash: {content_hash[:16]}...")
    
    async def detect_language(self, content: str) -> LanguageCode:
        """Detect the language of the input content."""
        try:
            prompt = f"""
            Detect the primary language of this content and respond with ONLY the language code:
            
            Content: "{content[:500]}"
            
            Respond with one of these codes: en, es, fr, de, it, pt, ru, zh, ja, ko, ar, hi, bn, te, ta, mr, kn
            If unsure, respond with: en
            """
            
            response = await self._generate_flash_response(prompt)
            detected_lang = response.strip().lower()
            
            # Validate the response
            try:
                return LanguageCode(detected_lang)
            except ValueError:
                return LanguageCode.EN  # Default fallback
                
        except Exception as e:
            logger.warning(f"Language detection failed: {str(e)}, defaulting to English")
            return LanguageCode.EN
    
    async def extract_claims(self, content: str, language: LanguageCode) -> List[ClaimExtraction]:
        """Extract key claims from content using Gemini Flash."""
        try:
            prompt = f"""
            You are an expert claim extraction system. Extract key factual claims from this content.
            
            Content Language: {language.value}
            Content: "{content}"
            
            Extract claims that can be fact-checked. For each claim, identify who, what, where, when if possible.
            
            Respond ONLY with valid JSON in this format:
            {{
                "claims": [
                    {{
                        "claim_text": "Specific factual claim",
                        "who": "Person/entity making claim (or null)",
                        "what": "What is being claimed",
                        "where": "Location if relevant (or null)",
                        "when": "Time period if relevant (or null)",
                        "confidence": 0.95
                    }}
                ]
            }}
            
            Guidelines:
            - Extract 1-5 most significant factual claims
            - Ignore opinions, speculation, or obvious facts
            - Focus on claims that can be verified
            - Set confidence based on how clear the claim is
            """
            
            response = await self._generate_flash_response(prompt)
            data = json.loads(response)
            
            claims = []
            for claim_data in data.get("claims", []):
                claim = ClaimExtraction(
                    claim_text=claim_data.get("claim_text", ""),
                    who=claim_data.get("who"),
                    what=claim_data.get("what"),
                    where=claim_data.get("where"),
                    when=claim_data.get("when"),
                    confidence=float(claim_data.get("confidence", 0.5))
                )
                claims.append(claim)
            
            logger.info(f"âœ… Extracted {len(claims)} claims from content")
            return claims
            
        except Exception as e:
            logger.error(f"âŒ Claim extraction failed: {str(e)}")
            return []
    
    async def retrieve_evidence(self, claims: List[ClaimExtraction]) -> EvidenceRetrievalResult:
        """Retrieve evidence from multiple sources for the extracted claims."""
        start_time = time.time()
        all_citations = []
        sources_searched = []
        
        try:
            # Search Google Fact Check Tools API if available
            if self.fact_check_api_key and self.fact_check_api_key != "local-fact-check-key":
                fact_check_citations = await self._search_fact_check_api(claims)
                all_citations.extend(fact_check_citations)
                sources_searched.append("Google Fact Check Tools API")
            
            # Search additional curated sources
            curated_citations = await self._search_curated_sources(claims)
            all_citations.extend(curated_citations)
            sources_searched.append("Curated Fact-Checking Sources")
            
            # Remove duplicates and sort by relevance
            unique_citations = self._deduplicate_citations(all_citations)
            sorted_citations = sorted(unique_citations, key=lambda x: x.relevance_score, reverse=True)
            
            # Limit to top citations
            top_citations = sorted_citations[:10]
            
            search_time = time.time() - start_time
            
            return EvidenceRetrievalResult(
                query="; ".join([claim.claim_text for claim in claims]),
                sources_searched=sources_searched,
                citations_found=top_citations,
                search_time=search_time,
                total_results=len(all_citations)
            )
            
        except Exception as e:
            logger.error(f"âŒ Evidence retrieval failed: {str(e)}")
            return EvidenceRetrievalResult(
                query="",
                sources_searched=[],
                citations_found=[],
                search_time=time.time() - start_time,
                total_results=0
            )
    
    async def _search_fact_check_api(self, claims: List[ClaimExtraction]) -> List[EvidenceCitation]:
        """Search Google Fact Check Tools API for relevant fact-checks."""
        citations = []
        
        try:
            for claim in claims[:3]:  # Limit to first 3 claims to avoid rate limits
                query = claim.claim_text[:100]  # Limit query length
                url = f"https://factchecktools.googleapis.com/v1alpha1/claims:search"
                params = {
                    "query": query,
                    "key": self.fact_check_api_key,
                    "languageCode": "en",
                    "maxAgeDays": 365,
                    "pageSize": 5
                }
                
                response = await self.http_client.get(url, params=params)
                
                if response.status_code == 200:
                    data = response.json()
                    
                    for claim_review in data.get("claims", []):
                        for review in claim_review.get("claimReview", []):
                            citation = EvidenceCitation(
                                title=review.get("title", "Fact Check Review"),
                                url=review.get("url", "https://example.com"),
                                snippet=claim_review.get("text", "")[:200],
                                date=self._parse_date(review.get("reviewDate")),
                                source_type="fact_check",
                                relevance_score=0.9,  # High relevance for fact-check sources
                                credibility_weight=0.95,
                                recency_weight=self._calculate_recency_weight(review.get("reviewDate"))
                            )
                            citations.append(citation)
                
                # Add delay to respect rate limits
                await asyncio.sleep(0.5)
        
        except Exception as e:
            logger.warning(f"Fact Check API search failed: {str(e)}")
        
        return citations
    
    async def _search_curated_sources(self, claims: List[ClaimExtraction]) -> List[EvidenceCitation]:
        """Search curated fact-checking sources using web search simulation."""
        citations = []
        
        # Simulate citations from reliable sources
        # In production, this would integrate with actual search APIs or databases
        reliable_sources = [
            {
                "domain": "snopes.com",
                "name": "Snopes",
                "credibility": 0.9,
                "type": "fact_check"
            },
            {
                "domain": "factcheck.org",
                "name": "FactCheck.org",
                "credibility": 0.95,
                "type": "fact_check"
            },
            {
                "domain": "politifact.com",
                "name": "PolitiFact",
                "credibility": 0.9,
                "type": "fact_check"
            },
            {
                "domain": "reuters.com",
                "name": "Reuters",
                "credibility": 0.85,
                "type": "news"
            },
            {
                "domain": "apnews.com",
                "name": "Associated Press",
                "credibility": 0.9,
                "type": "news"
            }
        ]
        
        try:
            for i, claim in enumerate(claims[:2]):  # Limit processing
                for source in reliable_sources[:3]:  # Use top 3 sources
                    # Simulate finding relevant content
                    citation = EvidenceCitation(
                        title=f"Analysis of claim about {claim.what or 'topic'}",
                        url=f"https://{source['domain']}/fact-check-{i+1}",
                        snippet=f"Our analysis shows that claims about {claim.what or 'this topic'} require additional context...",
                        date=datetime.now() - timedelta(days=30),
                        source_type=source["type"],
                        relevance_score=0.7 + (i * 0.1),
                        credibility_weight=source["credibility"],
                        recency_weight=0.8
                    )
                    citations.append(citation)
        
        except Exception as e:
            logger.warning(f"Curated source search failed: {str(e)}")
        
        return citations
    
    def _deduplicate_citations(self, citations: List[EvidenceCitation]) -> List[EvidenceCitation]:
        """Remove duplicate citations based on URL."""
        seen_urls = set()
        unique_citations = []
        
        for citation in citations:
            url_str = str(citation.url)
            if url_str not in seen_urls:
                seen_urls.add(url_str)
                unique_citations.append(citation)
        
        return unique_citations
    
    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """Parse date string to datetime object."""
        if not date_str:
            return None
        
        try:
            # Try different date formats
            formats = ["%Y-%m-%d", "%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%SZ"]
            for fmt in formats:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue
        except Exception:
            pass
        
        return None
    
    def _calculate_recency_weight(self, date_str: Optional[str]) -> float:
        """Calculate recency weight based on publication date."""
        if not date_str:
            return 0.5  # Default weight for unknown dates
        
        pub_date = self._parse_date(date_str)
        if not pub_date:
            return 0.5
        
        days_old = (datetime.now() - pub_date).days
        
        # Exponential decay: newer content gets higher weight
        if days_old <= 30:
            return 1.0
        elif days_old <= 90:
            return 0.8
        elif days_old <= 365:
            return 0.6
        else:
            return 0.4
    
    async def analyze_stance(
        self, 
        claims: List[ClaimExtraction], 
        citations: List[EvidenceCitation]
    ) -> List[StanceAnalysis]:
        """Analyze the stance of evidence towards each claim."""
        stance_analyses = []
        
        try:
            for i, claim in enumerate(claims):
                # Get relevant citations for this claim
                relevant_citations = citations[:3]  # Use top 3 citations for analysis
                
                citation_summaries = []
                for citation in relevant_citations:
                    citation_summaries.append(f"- {citation.title}: {citation.snippet}")
                
                evidence_text = "\n".join(citation_summaries)
                
                prompt = f"""
                Analyze how the following evidence relates to this claim:
                
                CLAIM: "{claim.claim_text}"
                
                EVIDENCE:
                {evidence_text}
                
                Determine the stance of the evidence towards the claim and respond ONLY with valid JSON:
                {{
                    "stance": "supports|refutes|needs_context|insufficient",
                    "confidence": 0.85,
                    "evidence_strength": 0.9,
                    "reasoning": "Brief explanation of the stance analysis"
                }}
                
                Guidelines:
                - "supports": Evidence clearly supports the claim
                - "refutes": Evidence clearly contradicts the claim
                - "needs_context": Evidence partially supports but requires additional context
                - "insufficient": Not enough evidence to make a determination
                """
                
                response = await self._generate_flash_response(prompt)
                data = json.loads(response)
                
                stance_analysis = StanceAnalysis(
                    claim_id=f"claim_{i}",
                    stance=StanceType(data.get("stance", "insufficient")),
                    confidence=float(data.get("confidence", 0.5)),
                    evidence_strength=float(data.get("evidence_strength", 0.5)),
                    citations=relevant_citations
                )
                
                stance_analyses.append(stance_analysis)
        
        except Exception as e:
            logger.error(f"âŒ Stance analysis failed: {str(e)}")
        
        return stance_analyses
    
    async def generate_verdict(
        self, 
        claims: List[ClaimExtraction],
        stance_analyses: List[StanceAnalysis],
        citations: List[EvidenceCitation],
        use_pro_model: bool = False
    ) -> Dict[str, Any]:
        """Generate final verdict with credibility score and explanation."""
        try:
            # Prepare summary of analysis
            claim_summaries = []
            for i, claim in enumerate(claims):
                stance = stance_analyses[i] if i < len(stance_analyses) else None
                stance_info = f" (Stance: {stance.stance.value})" if stance else ""
                claim_summaries.append(f"- {claim.claim_text}{stance_info}")
            
            claims_text = "\n".join(claim_summaries)
            
            prompt = f"""
            You are an expert fact-checker. Provide a comprehensive verdict for this content analysis.
            
            CLAIMS ANALYZED:
            {claims_text}
            
            EVIDENCE SOURCES: {len(citations)} citations from fact-checkers and reliable sources
            
            Provide your verdict in this EXACT JSON format:
            {{
                "score": 75,
                "badge": "amber",
                "verdict": "Partly accurate but needs context",
                "explanation": "Detailed explanation of why this content might mislead or be credible",
                "manipulation_techniques": ["cherry_picking", "false_context"],
                "learn_card": {{
                    "title": "Digital Literacy Tip",
                    "content": "Educational content about recognizing this type of misinformation",
                    "tip": "One actionable sentence for users",
                    "category": "source_verification"
                }}
            }}
            
            SCORING CRITERIA:
            - 80-100: Highly credible (green badge)
            - 40-79: Needs context (amber badge)  
            - 0-39: Misleading (red badge)
            
            MANIPULATION TECHNIQUES: cherry_picking, false_context, deepfake, emotional_manipulation, strawman, false_dichotomy, ad_hominem, bandwagon, fear_mongering, false_cure, conspiracy_theory, out_of_context
            
            Keep verdict under 25 words. Make explanation clear and specific.
            """
            
            model = self.pro_model if use_pro_model else self.flash_model
            response = await self._generate_response_with_model(prompt, model)
            
            return json.loads(response)
            
        except Exception as e:
            logger.error(f"âŒ Verdict generation failed: {str(e)}")
            return {
                "score": 50,
                "badge": "amber",
                "verdict": "Analysis inconclusive",
                "explanation": "Unable to complete comprehensive analysis due to technical limitations.",
                "manipulation_techniques": [],
                "learn_card": {
                    "title": "Verification Reminder",
                    "content": "When automated analysis fails, manually verify claims with multiple reliable sources.",
                    "tip": "Always cross-check important claims with trusted fact-checkers.",
                    "category": "general_verification"
                }
            }
    
    async def _generate_flash_response(self, prompt: str) -> str:
        """Generate response using Gemini Flash model."""
        return await self._generate_response_with_model(prompt, self.flash_model)
    
    async def _generate_pro_response(self, prompt: str) -> str:
        """Generate response using Gemini Pro model."""
        return await self._generate_response_with_model(prompt, self.pro_model)
    
    async def _generate_response_with_model(self, prompt: str, model) -> str:
        """Generate response with specified model."""
        try:
            if model is None:
                raise ValueError("Model not initialized")
            
            response = model.generate_content(prompt)
            return response.text
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}")
            raise
    
    async def analyze_misinformation_enhanced(
        self, 
        request: MisinformationAnalysisRequest
    ) -> MisinformationAnalysisResponse:
        """
        Enhanced misinformation analysis with structured claim extraction,
        evidence retrieval, and stance analysis.
        """
        start_time = time.time()
        content_hash = self._generate_content_hash(request.content, request.content_type)
        
        # Check cache first
        cached_result = self._get_cached_analysis(content_hash)
        if cached_result:
            return cached_result
        
        try:
            # Step 1: Language Detection
            detected_language = await self.detect_language(request.content)
            logger.info(f"ðŸŒ Detected language: {detected_language.value}")
            
            # Step 2: Claim Extraction
            claims = await self.extract_claims(request.content, detected_language)
            logger.info(f"ðŸ“‹ Extracted {len(claims)} claims")
            
            # Step 3: Evidence Retrieval
            evidence_result = await self.retrieve_evidence(claims)
            logger.info(f"ðŸ” Found {len(evidence_result.citations_found)} citations")
            
            # Step 4: Stance Analysis
            stance_analyses = await self.analyze_stance(claims, evidence_result.citations_found)
            logger.info(f"âš–ï¸ Completed stance analysis for {len(stance_analyses)} claims")
            
            # Step 5: Determine if escalation to Pro model is needed
            should_escalate = self._should_escalate_to_pro(claims, stance_analyses, request.force_pro_model)
            
            # Step 6: Generate Verdict
            verdict_data = await self.generate_verdict(
                claims, 
                stance_analyses, 
                evidence_result.citations_found,
                use_pro_model=should_escalate
            )
            
            # Create response
            processing_time = time.time() - start_time
            model_used = ProcessingModel.GEMINI_PRO if should_escalate else ProcessingModel.GEMINI_FLASH
            
            result = MisinformationAnalysisResponse(
                content_id=f"analysis_{int(start_time)}",
                language=detected_language,
                processing_model=model_used,
                claims=claims,
                score=verdict_data["score"],
                badge=CredibilityBadgeType(verdict_data["badge"]),
                verdict=verdict_data["verdict"],
                stance_analyses=stance_analyses,
                citations=evidence_result.citations_found,
                explanation=verdict_data["explanation"],
                manipulation_techniques=[
                    ManipulationTechnique(tech) for tech in verdict_data.get("manipulation_techniques", [])
                ],
                learn_card=LearnCard(**verdict_data["learn_card"]),
                processing_time=processing_time,
                model_escalated=should_escalate,
                cache_hit=False,
                created_at=datetime.now()
            )
            
            # Cache the result
            self._cache_analysis(content_hash, result)
            
            logger.info(f"âœ… Enhanced analysis completed in {processing_time:.2f}s (Score: {result.score}, Badge: {result.badge.value})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Enhanced misinformation analysis failed: {str(e)}")
            
            # Return default safe response
            processing_time = time.time() - start_time
            return MisinformationAnalysisResponse(
                content_id=f"error_{int(start_time)}",
                language=LanguageCode.EN,
                processing_model=ProcessingModel.GEMINI_FLASH,
                claims=[],
                score=50,
                badge=CredibilityBadgeType.AMBER,
                verdict="Analysis incomplete - verify manually",
                stance_analyses=[],
                citations=[],
                explanation="Technical issues prevented complete analysis. Please verify claims manually with trusted sources.",
                manipulation_techniques=[],
                learn_card=LearnCard(
                    title="Technical Limitation",
                    content="When automated systems fail, human verification becomes essential for important claims.",
                    tip="Always have backup verification methods for critical information.",
                    category="system_reliability"
                ),
                processing_time=processing_time,
                model_escalated=False,
                cache_hit=False,
                created_at=datetime.now()
            )
    
    def _should_escalate_to_pro(
        self, 
        claims: List[ClaimExtraction], 
        stance_analyses: List[StanceAnalysis],
        force_pro: bool = False
    ) -> bool:
        """Determine if analysis should be escalated from Flash to Pro model."""
        if force_pro:
            return True
        
        # Escalate if claims are complex or ambiguous
        if len(claims) > 3:
            return True
        
        # Escalate if stance analysis shows conflicting evidence
        conflicting_stances = sum(1 for stance in stance_analyses 
                                if stance.stance == StanceType.NEEDS_CONTEXT)
        if conflicting_stances > 1:
            return True
        
        # Escalate if confidence is low
        avg_confidence = sum(stance.confidence for stance in stance_analyses) / len(stance_analyses) if stance_analyses else 0
        if avg_confidence < 0.7:
            return True
        
        return False
    
    # Legacy methods for backward compatibility
    def generate_embeddings(self, text: str) -> Optional[List[float]]:
        """
        Generate embeddings for the given text using Gemini embedding model.
        This helps in understanding context and similarity better.
        """
        try:
            if self.embedding_model is None:
                logger.warning("âš ï¸ Embedding model not initialized")
                return None
                
            result = genai.embed_content(
                model=self.embedding_model,
                content=text,
                task_type="retrieval_document",
                title="Content Analysis"
            )
            return result['embedding']
        except Exception as e:
            logger.error(f"âŒ Failed to generate embeddings: {str(e)}")
            return None
    
    async def fact_check_fallback(
        self, 
        content: str, 
        content_type: str = "text",
        context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Enhanced fallback fact checking using Gemini API when other services fail.
        Uses embeddings for better context understanding and returns structured response.
        """
        if self.flash_model is None:
            logger.error("âŒ Gemini model not initialized")
            return {
                "success": False,
                "error": "Gemini model not initialized",
                "source": "gemini_fallback"
            }
        
        try:
            # Generate embeddings for better context if content is substantial
            embeddings = None
            if len(content) > 50:  # Use embeddings for substantial content
                embeddings = self.generate_embeddings(content)
                if embeddings:
                    logger.info(f"âœ… Generated embeddings with dimension: {len(embeddings)}")
            
            # Enhanced fact-checking prompt with structured output
            prompt = self._build_fallback_analysis_prompt(content, content_type, context, embeddings)
            
            # Generate response from Gemini
            response = await self._generate_flash_response(prompt)
            
            # Parse and format the response
            analysis_result = self._parse_fallback_response(response, content)
            
            logger.info(f"âœ… Gemini fallback analysis completed for content type: {content_type}")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ Gemini fallback analysis failed: {str(e)}")
            return {
                "success": False,
                "error": f"Fallback analysis failed: {str(e)}",
                "source": "gemini_fallback"
            }
    
    def _build_fallback_analysis_prompt(
        self, 
        content: str, 
        content_type: str,
        context: Optional[Dict[str, Any]] = None,
        embeddings: Optional[List[float]] = None
    ) -> str:
        """Build enhanced analysis prompt for fallback fact-checking."""
        
        # Add context information if available
        context_info = ""
        if context:
            context_info = f"""
        Additional Context:
        - User location: {context.get('location', 'Unknown')}
        - Language preference: {context.get('language', 'en')}
        - Content source: {context.get('source', 'Unknown')}
        """
        
        # Add embedding information if available
        embedding_info = ""
        if embeddings:
            embedding_info = f"""
        Content Embeddings: Generated ({len(embeddings)} dimensions) - This content has been analyzed for semantic similarity and context.
        """
        
        prompt = f"""
        You are an expert AI fact-checker with extensive knowledge of current events, common misinformation patterns, and verification techniques. 
        
        CRITICAL INSTRUCTION: Respond ONLY with valid JSON. No additional text before or after the JSON.
        
        {context_info}
        {embedding_info}
        
        CONTENT TO ANALYZE ({content_type}):
        ---
        {content}
        ---
        
        Provide a comprehensive fact-check analysis in this EXACT JSON format:
        {{
            "success": true,
            "source": "gemini_fallback",
            "credibility_score": <float between 0.0 and 1.0>,
            "verdict": "<one of: ACCURATE, MOSTLY_ACCURATE, MIXED, MOSTLY_INACCURATE, INACCURATE, UNVERIFIABLE>",
            "confidence": <float between 0.0 and 1.0>,
            "summary": "<brief 2-3 sentence summary of your analysis>",
            "detailed_analysis": {{
                "key_findings": [
                    "<finding 1>",
                    "<finding 2>",
                    "<finding 3>"
                ],
                "red_flags": [
                    "<potential issue 1>",
                    "<potential issue 2>"
                ],
                "supporting_evidence": [
                    "<evidence 1>",
                    "<evidence 2>"
                ],
                "reasoning": "<detailed explanation of your analysis process>"
            }},
            "citations": [
                {{
                    "title": "<reliable source title>",
                    "url": "<source URL>",
                    "snippet": "<relevant excerpt>",
                    "relevance_score": <float between 0.0 and 1.0>,
                    "source_type": "<news/academic/government/fact_check>"
                }}
            ],
            "recommendations": [
                "<actionable recommendation 1>",
                "<actionable recommendation 2>"
            ],
            "metadata": {{
                "processing_method": "gemini_fallback",
                "content_type": "{content_type}",
                "analysis_timestamp": "{time.time()}",
                "embedding_used": {embeddings is not None}
            }}
        }}
        
        ANALYSIS GUIDELINES:
        1. Assess factual accuracy based on your knowledge cutoff
        2. Identify logical fallacies, emotional manipulation, or bias
        3. Check for outdated information or missing context
        4. Look for signs of manipulation (deepfakes, selective editing, etc.)
        5. Consider the source credibility if identifiable
        6. Provide specific, actionable recommendations
        
        SCORING CRITERIA:
        - credibility_score: 0.0-0.3 (Highly suspicious), 0.3-0.6 (Questionable), 0.6-0.8 (Generally reliable), 0.8-1.0 (Highly credible)
        - confidence: How certain you are about your assessment
        - relevance_score: How relevant each citation is to the analysis
        
        Remember: If you cannot verify specific claims, mark as UNVERIFIABLE rather than making assumptions.
        """
        
        return prompt
    
    def _parse_fallback_response(self, response_text: str, original_content: str) -> Dict[str, Any]:
        """Parse and format the Gemini fallback response."""
        try:
            # Try to parse as JSON first
            if response_text.strip().startswith('{'):
                import json
                parsed = json.loads(response_text)
                return parsed
            
            # If not JSON, create structured response from text
            return {
                "success": True,
                "source": "gemini_fallback",
                "credibility_score": 0.5,  # Default moderate score
                "verdict": "UNVERIFIABLE",
                "confidence": 0.7,
                "summary": response_text[:200] + "..." if len(response_text) > 200 else response_text,
                "detailed_analysis": {
                    "key_findings": [response_text],
                    "red_flags": [],
                    "supporting_evidence": [],
                    "reasoning": "Analysis provided by Gemini AI fallback system"
                },
                "citations": [],
                "recommendations": ["Verify with additional sources", "Check for recent updates"],
                "metadata": {
                    "processing_method": "gemini_fallback",
                    "content_type": "text",
                    "analysis_timestamp": time.time(),
                    "embedding_used": False
                }
            }
        except Exception as e:
            logger.error(f"âŒ Failed to parse fallback response: {str(e)}")
            return {
                "success": False,
                "error": f"Response parsing failed: {str(e)}",
                "source": "gemini_fallback"
            }
    
    async def analyze_text_content(
        self, 
        content: str, 
        content_type: ContentType,
        language: str = "en"
    ) -> ContentAnalysisResponse:
        """
        Analyze text content for misinformation using Gemini AI.
        
        Args:
            content: The text content to analyze
            content_type: Type of content (text, link)
            language: Language of the content
            
        Returns:
            ContentAnalysisResponse with analysis results
        """
        start_time = time.time()
        
        try:
            # Prepare the prompt for analysis
            prompt = self._build_analysis_prompt(content, content_type, language)
            
            # Generate response from Gemini
            response = await self._generate_flash_response(prompt)
            
            # Parse the response
            analysis_result = self._parse_analysis_response(response)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Generate content ID
            content_id = f"content_{int(start_time)}"
            
            return ContentAnalysisResponse(
                content_id=content_id,
                original_content=content,
                detected_language=language,
                misinformation_level=analysis_result["misinformation_level"],
                reliability_score=analysis_result["reliability_score"],
                explanation=analysis_result["explanation"],
                sources=analysis_result.get("sources", []),
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error analyzing text content: {str(e)}")
            raise
    
    async def analyze_image_content(
        self, 
        image_data: bytes,
        additional_context: Optional[str] = None
    ) -> ContentAnalysisResponse:
        """
        Analyze image content for misinformation using Gemini Vision.
        
        Args:
            image_data: Raw image bytes
            additional_context: Additional text context if available
            
        Returns:
            ContentAnalysisResponse with analysis results
        """
        start_time = time.time()
        
        try:
            # Convert bytes to PIL Image
            image = Image.open(io.BytesIO(image_data))
            
            # Prepare the prompt for image analysis
            prompt = self._build_image_analysis_prompt(additional_context)
            
            # Generate response from Gemini Vision
            response = await self._generate_vision_response(prompt, image)
            
            # Parse the response
            analysis_result = self._parse_analysis_response(response)
            
            # Calculate processing time
            processing_time = time.time() - start_time
            
            # Generate content ID
            content_id = f"image_{int(start_time)}"
            
            return ContentAnalysisResponse(
                content_id=content_id,
                original_content="[Image Content]",
                detected_language="en",  # Default for images
                misinformation_level=analysis_result["misinformation_level"],
                reliability_score=analysis_result["reliability_score"],
                explanation=analysis_result["explanation"],
                sources=analysis_result.get("sources", []),
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Error analyzing image content: {str(e)}")
            raise
    
    def _build_analysis_prompt(
        self, 
        content: str, 
        content_type: ContentType,
        language: str
    ) -> str:
        """Build the analysis prompt for Gemini AI."""
        
        base_prompt = f"""
        You are an expert fact-checker and misinformation detection specialist. 
        Analyze the following {content_type.value} content for potential misinformation.
        
        Content Language: {language}
        Content Type: {content_type.value}
        
        Content to analyze:
        "{content}"
        
        Please provide a comprehensive analysis in the following JSON format:
        {{
            "misinformation_level": "low|medium|high|critical",
            "reliability_score": 0.0-1.0,
            "explanation": {{
                "reasoning": "Detailed explanation of why this content was flagged",
                "key_indicators": ["indicator1", "indicator2", "indicator3"],
                "confidence_score": 0.0-1.0,
                "suggested_actions": ["action1", "action2", "action3"]
            }},
            "sources": [
                {{
                    "title": "Source title",
                    "url": "https://source-url.com",
                    "description": "Brief description",
                    "reliability_score": 0.0-1.0
                }}
            ]
        }}
        
        Guidelines for analysis:
        1. Check for sensationalist language, unverified claims, or emotional manipulation
        2. Look for missing context, cherry-picked facts, or logical fallacies
        3. Identify potential bias, conspiracy theories, or pseudoscience
        4. Consider the source credibility and fact-checking history
        5. Provide specific, actionable feedback for users
        
        Be thorough but fair in your analysis. If the content appears legitimate, 
        indicate a low misinformation level with high reliability score.
        """
        
        return base_prompt
    
    def _build_image_analysis_prompt(self, additional_context: Optional[str] = None) -> str:
        """Build the image analysis prompt for Gemini Vision."""
        
        context_part = f"\nAdditional Context: {additional_context}" if additional_context else ""
        
        prompt = f"""
        You are an expert fact-checker analyzing an image for potential misinformation.
        {context_part}
        
        Please analyze this image and provide a comprehensive assessment in the following JSON format:
        {{
            "misinformation_level": "low|medium|high|critical",
            "reliability_score": 0.0-1.0,
            "explanation": {{
                "reasoning": "Detailed explanation of what you see and why it was flagged",
                "key_indicators": ["indicator1", "indicator2", "indicator3"],
                "confidence_score": 0.0-1.0,
                "suggested_actions": ["action1", "action2", "action3"]
            }},
            "sources": [
                {{
                    "title": "Source title",
                    "url": "https://source-url.com",
                    "description": "Brief description",
                    "reliability_score": 0.0-1.0
                }}
            ]
        }}
        
        Consider:
        1. Visual manipulation, deepfakes, or edited images
        2. Misleading captions or context
        3. Outdated or misattributed images
        4. Emotional manipulation through imagery
        5. Missing context or selective framing
        
        Describe what you see in the image and explain your reasoning clearly.
        """
        
        return prompt
    
    async def _generate_vision_response(self, prompt: str, image: Image.Image) -> str:
        """Generate response from Gemini vision model."""
        try:
            response = self.vision_model.generate_content([prompt, image])
            return response.text
        except Exception as e:
            logger.error(f"Error generating Gemini vision response: {str(e)}")
            raise
    
    def _parse_analysis_response(self, response: str) -> Dict[str, Any]:
        """Parse the Gemini response into structured data."""
        try:
            import json
            
            # Extract JSON from response (handle cases where response includes extra text)
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("No JSON found in response")
            
            json_str = response[start_idx:end_idx]
            data = json.loads(json_str)
            
            # Validate and structure the response
            misinformation_level = MisinformationLevel(data.get("misinformation_level", "low"))
            reliability_score = float(data.get("reliability_score", 0.5))
            
            explanation_data = data.get("explanation", {})
            explanation = DetectionExplanation(
                reasoning=explanation_data.get("reasoning", "No reasoning provided"),
                key_indicators=explanation_data.get("key_indicators", []),
                confidence_score=float(explanation_data.get("confidence_score", 0.5)),
                suggested_actions=explanation_data.get("suggested_actions", [])
            )
            
            sources = []
            for source_data in data.get("sources", []):
                source = SourceInfo(
                    title=source_data.get("title", ""),
                    url=source_data.get("url", "https://example.com"),
                    description=source_data.get("description", ""),
                    reliability_score=float(source_data.get("reliability_score", 0.5))
                )
                sources.append(source)
            
            return {
                "misinformation_level": misinformation_level,
                "reliability_score": reliability_score,
                "explanation": explanation,
                "sources": sources
            }
            
        except Exception as e:
            logger.error(f"Error parsing analysis response: {str(e)}")
            # Return default response if parsing fails
            return {
                "misinformation_level": MisinformationLevel.LOW,
                "reliability_score": 0.5,
                "explanation": DetectionExplanation(
                    reasoning="Unable to parse AI response",
                    key_indicators=["Analysis failed"],
                    confidence_score=0.0,
                    suggested_actions=["Please try again or contact support"]
                ),
                "sources": []
            }
    
    async def generate_learning_content(self, topic: str) -> str:
        """Generate educational content about misinformation detection."""
        prompt = f"""
        Create an educational module about "{topic}" related to misinformation detection.
        Make it engaging, informative, and suitable for general audiences.
        Include practical tips and examples.
        """
        
        try:
            response = await self._generate_flash_response(prompt)
            return response
        except Exception as e:
            logger.error(f"Error generating learning content: {str(e)}")
            return f"Unable to generate content about {topic}. Please try again later."
    
    async def generate_quiz_questions(self, topic: str, num_questions: int = 5) -> List[Dict[str, Any]]:
        """Generate quiz questions for learning modules."""
        prompt = f"""
        Generate {num_questions} multiple-choice questions about "{topic}" 
        related to misinformation detection. Format as JSON:
        {{
            "questions": [
                {{
                    "question": "Question text",
                    "options": ["A", "B", "C", "D"],
                    "correct_answer": 0,
                    "explanation": "Why this is correct"
                }}
            ]
        }}
        """
        
        try:
            response = await self._generate_flash_response(prompt)
            # Parse response and extract questions
            import json
            data = json.loads(response)
            return data.get("questions", [])
        except Exception as e:
            logger.error(f"Error generating quiz questions: {str(e)}")
            return []


# Global instances for backward compatibility
gemini_service = EnhancedGeminiService()
enhanced_gemini_service = gemini_service  # Alias for clarity
